set.seed(6379)
myData = read.csv("Coding2_myData.csv")
X = as.matrix(myData[, -14])
y = myData$Y
dim(X)
set.seed(6379)
myData = read.csv("Coding2_myData.csv")
X = as.matrix(myData[, -14])
y = myData$Y
dim(X)
View(X)
View(X)
View(X)
View(myData)
one_var_lasso = function(r, x, lam) {
xx = sum(x^2)
xr = sum(r * x)
b = (abs(xr) - lam/2)/xx
b = sign(xr) * ifelse(b > 0, b, 0)
return(b)
}
glmnet_sd = function (z, n) {
sd(z) * sqrt((n-1)/n)
}
MyLasso = function(X, y, lam.seq, maxit = 500) {
# X: n-by-p design matrix without the intercept
# y: n-by-1 response vector
# lam.seq: sequence of lambda values
# maxit: number of updates for each lambda
# Center/Scale X
# Center y
n = length(y)
p = dim(X)[2]
nlam = length(lam.seq)
##############################
# YOUR CODE:
# Record the corresponding means and scales
# For example,
# y.mean = mean(y)
# Xs = centered and scaled X
##############################
# Center y
y.mean = mean(y)
# Center/Scale X while storing the mean and standard deviations for each predictor
x.column.means = colMeans(X)
x.column.sd_vals = apply(X, 2, glmnet_sd, n)
Xc = t(t(X) - x.column.means)
Xs = t(t(Xc) / x.column.sd_vals)
# Initialize coef vector b and residual vector r
b = rep(0, p)
r = y
B = matrix(nrow = nlam, ncol = p + 1)
# Triple nested loop
for (m in 1:nlam) {
lam = 2 * n * lam.seq[m]
for (step in 1:maxit) {
for (j in 1:p) {
r = r + (Xs[, j] * b[j])
b[j] = one_var_lasso(r, Xs[, j], lam)
r = r - Xs[, j] * b[j]
}
}
B[m, ] = c(0, b)
}
##############################
# YOUR CODE:
# Scale back the coefficients;
# Update the intercepts stored in B[, 1]
##############################
x.col_vals = x.column.means / x.column.sd_vals
# Update the intercepts stored in B[, 1]
B[, 1] = y.mean - B[,-1] %*% x.col_vals
# Scale back the coefficients;
B[, -1] = t(t(B[, -1]) / x.column.sd_vals)
# Return the transpose of all model coefficients for each lambda, including the Intercepts
t(B)
}
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq, maxit = 100)
rownames(myout) = c("Intercept", colnames(X))
dim(myout)
x.index = log(lam.seq)
beta = myout[-1, ]  # beta is a 13-by-80 matrix
matplot(x.index, t(beta),
xlim = c(min(x.index), max(x.index)),
lty = 1,
xlab = "Log Lambda",
ylab = "Coefficients",
type="l",
lwd = 1)
# add variable names to each path
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
write.csv(as.matrix(coef(lasso.fit)), file = "Coding2_lasso_coefs.csv",
row.names = FALSE)
max(abs(coef(lasso.fit) - myout))
plot(lasso.fit, xvar = "lambda")
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)
# add variable names to each path
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)
